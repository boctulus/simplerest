# LLM Providers

Package unificado para consumir APIs de distintos LLM (Large Language Models) con una interfaz consistente y extensible.

## Estado del Proyecto

**Versi√≥n**: 1.0.0
**Estado**: Producci√≥n (Funcional)
**√öltima actualizaci√≥n**: 2025-01

### Observaciones sobre Implementaci√≥n y Capacidades

#### ‚úÖ Ventajas del Package

1. **Interfaz Unificada**: Trabaja con m√∫ltiples proveedores LLM usando exactamente la misma API
2. **C√≥digo DRY**: Evita duplicaci√≥n de l√≥gica entre diferentes implementaciones de proveedores
3. **F√°cil Migraci√≥n**: Cambiar de un proveedor a otro requiere solo cambiar el nombre en el factory
4. **Extensibilidad**: Agregar nuevos proveedores es trivial (solo implementar la interfaz)
5. **Type-Safe**: Interfaces bien definidas con documentaci√≥n completa
6. **Manejo Robusto de Errores**: Excepciones personalizadas con detalles espec√≠ficos del proveedor
7. **Factory Pattern**: Instanciaci√≥n simplificada y consistente
8. **Mantenci√≥n Centralizada**: Bugs y mejoras se propagan a todos los proveedores

#### ‚ö†Ô∏è Limitaciones Actuales

1. **Funcionalidades Espec√≠ficas**: Algunas caracter√≠sticas √∫nicas de cada proveedor no est√°n expuestas en la interfaz com√∫n
   - OpenAI: An√°lisis de im√°genes, generaci√≥n de audio (Whisper), DALL-E
   - Claude: Streaming de respuestas, tool use (function calling)
   - Grok: M√©todos espec√≠ficos como `getSystemFingerprint()`, `getPromptTokensDetails()`

2. **Respuestas Heterog√©neas**: Aunque normalizadas, cada proveedor devuelve estructuras ligeramente diferentes
   - OpenAI: `finish_reason` puede ser 'stop', 'length', 'content_filter'
   - Claude: `stop_reason` puede ser 'end_turn', 'stop_sequence', 'max_tokens'
   - Esto est√° documentado pero requiere atenci√≥n del desarrollador

3. **Testing Limitado**: No incluye tests automatizados (se recomienda agregar PHPUnit tests)

4. **Sin Soporte para Streaming**: Actualmente no soporta respuestas en streaming (importante para UX en tiempo real)

#### üí∞ ¬øVale la Pena este package?

**S√ç, definitivamente vale la pena** si:

- ‚úÖ Trabajas con m√∫ltiples proveedores de LLM
- ‚úÖ Quieres flexibilidad para cambiar de proveedor sin reescribir c√≥digo
- ‚úÖ Prefieres una interfaz limpia y *consistente*
- ‚úÖ Necesitas centralizar la l√≥gica de manejo de errores
- ‚úÖ Planeas agregar m√°s proveedores en el futuro (Gemini, Mistral, etc.)
- ‚úÖ Valoras c√≥digo mantenible y testeable

**Considera alternativas** si:

- ‚ùå Solo usas un √∫nico proveedor y no planeas cambiar
- ‚ùå Necesitas funcionalidades muy espec√≠ficas de un proveedor particular
- ‚ùå Requieres streaming de respuestas (por ahora)
- ‚ùå Tu proyecto es extremadamente simple (1-2 llamadas a API)

#### üöÄ Mejoras Futuras Recomendadas

1. Agregar soporte para streaming de respuestas
2. Implementar tests unitarios con PHPUnit
3. Agregar m√°s proveedores (Gemini, Mistral, Llama, etc.)
4. Soporte para embeddings de manera unificada
5. Cache inteligente de respuestas con TTL configurable
6. Logging estructurado de peticiones/respuestas
7. Rate limiting configurable por proveedor
8. Retry autom√°tico con backoff exponencial

---

## Caracter√≠sticas

- **Interfaz unificada**: Trabaja con m√∫ltiples proveedores usando la misma API
- **Extensible**: F√°cil de agregar nuevos proveedores
- **Factory Pattern**: Instanciaci√≥n simplificada de providers
- **Type-safe**: Interfaces bien definidas y documentadas
- **Manejo de errores**: Excepciones personalizadas con informaci√≥n detallada

## Proveedores Soportados

| Proveedor | Alias | Estado | Modelos Principales |
|-----------|-------|--------|---------------------|
| **OpenAI** | `openai`, `chatgpt`, `gpt` | ‚úÖ Completo | GPT-4, GPT-4o, GPT-3.5, DALL-E, Whisper |
| **Claude AI** | `claude`, `anthropic` | ‚úÖ Completo | Claude 3 Opus/Sonnet/Haiku, Claude 2 |
| **Grok** | `grok`, `xai` | ‚úÖ Completo | Grok Beta |
| **Ollama** | `ollama` | ‚úÖ Completo | Modelos locales (Llama, Mistral, etc) |

## Instalaci√≥n

El package ya est√° incluido en Simplerest bajo `packages/boctulus/llm-providers`.

Actualiza el autoload de Composer:

```bash
composer dumpautoload
```

## Configuraci√≥n

Configura tus API keys en el archivo de configuraci√≥n:

```php
// config/config.php
return [
    'openai_api_key' => 'sk-...',
    'claude_api_key' => 'sk-ant-...',
    'xai_api_key' => 'xai-...',
];
```

O mediante el archivo de configuraci√≥n del package:

```php
// packages/boctulus/llm-providers/config/config.php
return [
    'default_provider' => 'openai',

    'providers' => [
        'openai' => [
            'api_key' => env('OPENAI_API_KEY'),
            'default_model' => 'gpt-4o-mini',
        ],
        'claude' => [
            'api_key' => env('CLAUDE_API_KEY'),
            'api_version' => '2023-06-01',
            'default_model' => 'claude-3-sonnet-20240229',
        ],
        'grok' => [
            'api_key' => env('XAI_API_KEY'),
            'default_model' => 'grok-beta',
        ],
        'ollama' => [
            'default_model' => 'llama2', // O cualquier otro modelo que tengas
        ],
    ],
];
```

## Uso B√°sico

### Usando el Factory

```php
use Boctulus\LLMProviders\Factory\LLMFactory;

// Crear proveedor de OpenAI
$llm = LLMFactory::create('openai');

// Crear proveedor de Claude
$llm = LLMFactory::create('claude');

// Crear proveedor de Grok
$llm = LLMFactory::create('grok');

// Crear proveedor de Ollama
$llm = LLMFactory::create('ollama');

// O usar m√©todos est√°ticos directos
$openai = LLMFactory::openai();
$claude = LLMFactory::claude();
$grok   = LLMFactory::grok();
$ollama = LLMFactory::ollama();

// Con API key personalizada
$llm = LLMFactory::create('openai', ['api_key' => 'sk-...']);
```

### Ejemplo con OpenAI

```php
use Boctulus\LLMProviders\Factory\LLMFactory;

// Crear instancia
$llm = LLMFactory::openai();

// Configurar modelo y par√°metros
$llm->setModel('gpt-4o-mini')
    ->setMaxTokens(500)
    ->setTemperature(0.7);

// Agregar contenido
$llm->addContent('¬øCu√°l es la capital de Francia?');

// Ejecutar
$response = $llm->exec();

// Obtener contenido de la respuesta
if ($response['status'] == 200) {
    echo $llm->getContent();

    // Informaci√≥n adicional
    $tokens = $llm->getTokenUsage();
    $isComplete = $llm->isComplete();
}
```

### Ejemplo con Claude AI

```php
use Boctulus\LLMProviders\Factory\LLMFactory;

$llm = LLMFactory::claude();

$llm->setModel('claude-3-sonnet-20240229')
    ->setParams(['max_tokens' => 200]);

$llm->addContent('Explica la teor√≠a de la relatividad en t√©rminos simples');

$response = $llm->exec();

if ($response['status'] == 200) {
    echo $llm->getContent();

    // Verificar si la respuesta est√° completa
    if ($llm->isComplete()) {
        echo "\n‚úÖ Respuesta completa";
    }
}
```

### Ejemplo con Grok

```php
use Boctulus\LLMProviders\Factory\LLMFactory;

$llm = LLMFactory::grok();

$llm->setModel('grok-beta')
    ->setTemperature(0.5)
    ->setMaxTokens(300);

$llm->addContent('¬øQu√© es la computaci√≥n cu√°ntica?');

$response = $llm->exec();

if ($response['status'] == 200) {
    echo $llm->getContent();

    // M√©todos espec√≠ficos de Grok
    echo "\nModel: " . $llm->getModelName();
    echo "\nResponse ID: " . $llm->getResponseId();
}
```

### Ejemplo con Ollama

```php
use Boctulus\LLMProviders\Factory\LLMFactory;

$llm = LLMFactory::ollama();

// Listar modelos locales
$models = $llm->listModels();
print_r($models);

// Usar un modelo espec√≠fico
$llm->setModel('llama2') // o 'mistral', 'codellama', etc.
    ->addContent('Escribe un poema sobre la programaci√≥n');

$response = $llm->exec();

if ($response['status'] == 200) {
    echo $llm->getContent();
}
```

### Conversaciones Multi-turno

```php
$llm = LLMFactory::openai();

// Agregar contexto del sistema
$llm->addContent('Eres un asistente experto en f√≠sica.', 'system');

// Agregar mensaje del usuario
$llm->addContent('¬øQu√© es un agujero negro?', 'user');

// Ejecutar
$response = $llm->exec();
$respuesta1 = $llm->getContent();

// Continuar la conversaci√≥n
$llm->addContent('¬øY c√≥mo se forma?', 'user');
$response = $llm->exec();
$respuesta2 = $llm->getContent();
```

### Extracci√≥n de JSON

```php
$llm = LLMFactory::openai();

$llm->addContent('
    Dame una lista de 3 pa√≠ses europeos con sus capitales en formato JSON.
    Devuelve el JSON dentro de bloques ```json
');

$response = $llm->exec();

// getContent(true) autom√°ticamente decodifica JSON
$data = $llm->getContent(true);

// $data ahora es un array PHP
print_r($data);
/*
Array (
    [0] => Array (
        [pais] => Francia
        [capital] => Par√≠s
    )
    ...
)
*/
```

### Cambiar de Proveedor F√°cilmente

```php
// Esta funci√≥n funciona con CUALQUIER proveedor
function askLLM($providerName, $question) {
    $llm = LLMFactory::create($providerName);
    $llm->addContent($question);
    $response = $llm->exec();

    return $llm->getContent();
}

// Usar diferentes proveedores
$respuesta1 = askLLM('openai', '¬øQu√© es PHP?');
$respuesta2 = askLLM('claude', '¬øQu√© es PHP?');
$respuesta3 = askLLM('grok', '¬øQu√© es PHP?');

// Comparar respuestas de diferentes LLMs
```

## M√©todos de la Interfaz

### Configuraci√≥n

- `setModel(string $name)`: Configura el modelo a usar
- `getModel(): string`: Obtiene el modelo actual
- `setMaxTokens(int $val)`: Configura el l√≠mite de tokens
- `getMaxTokens(): ?int`: Obtiene el l√≠mite de tokens
- `setTemperature(float $val)`: Configura la temperatura (0.0 - 2.0)
- `setParams(array $params)`: Configura par√°metros adicionales

### Contenido

- `addContent(string $content, string $role = 'user')`: Agrega un mensaje
- `exec(?string $model = null): array`: Ejecuta la solicitud

### Respuesta

- `getContent(bool $decode = true)`: Obtiene el contenido de la respuesta
- `getTokenUsage(): ?array`: Obtiene informaci√≥n de uso de tokens
- `getFinishReason(): ?string`: Obtiene la raz√≥n de finalizaci√≥n
- `isComplete(): bool`: Verifica si la respuesta est√° completa
- `wereTokenEnough(): bool`: Verifica si hubo suficientes tokens
- `error()`: Obtiene el mensaje de error si lo hay

### Cliente

- `getClient()`: Obtiene el cliente HTTP subyacente

## Modelos Soportados

### OpenAI

#### Recomendados
- `gpt-4o-mini` ‚≠ê (econ√≥mico, excelente relaci√≥n calidad-precio)
- `gpt-4o` (potente, con capacidad de visi√≥n)
- `gpt-4` (muy potente, costoso)

#### Otros
- Serie O1: `o1-preview`, `o1-mini` (razonamiento avanzado)
- Serie GPT-3.5: `gpt-3.5-turbo-1106` (ahora m√°s caro que gpt-4o-mini)
- Im√°genes: `dall-e-3`, `dall-e-2`
- Audio: `whisper-1`, `tts-1`, `tts-1-hd`

### Claude AI

- `claude-3-opus-20240229` (m√°s potente, costoso)
- `claude-3-sonnet-20240229` ‚≠ê (balance ideal)
- `claude-3-haiku-20240307` (m√°s r√°pido, econ√≥mico)
- `claude-2.1`, `claude-2.0` (versiones anteriores)
- `claude-instant-1.2` (r√°pido y econ√≥mico)

### Grok (X.AI)

- `grok-beta` (modelo principal)

### Ollama

Cualquier modelo instalado localmente, por ejemplo:
- `llama2`
- `mistral`
- `codellama`
- `phi`

## Manejo de Errores

```php
use Boctulus\LLMProviders\Exceptions\ProviderException;

try {
    $llm = LLMFactory::create('openai');
    $llm->addContent('Test');
    $response = $llm->exec();

    // Verificar errores de la API
    if ($error = $llm->error()) {
        echo "Error del proveedor: ";
        print_r($error);
    }

    // Verificar respuesta HTTP
    if ($response['status'] !== 200) {
        echo "Error HTTP: " . $response['status'];
    }

} catch (ProviderException $e) {
    echo "Error: " . $e->getMessage() . "\n";
    echo "Provider: " . $e->getProvider() . "\n";
    print_r($e->getDetails());
}
```

### Razones de Finalizaci√≥n

Cada proveedor usa diferentes c√≥digos para indicar por qu√© termin√≥ la generaci√≥n:

#### OpenAI / Grok
- `stop`: Respuesta completa y natural ‚úÖ
- `length`: Se alcanz√≥ el l√≠mite de tokens ‚ö†Ô∏è
- `content_filter`: Bloqueado por filtros de contenido üö´

#### Claude
- `end_turn`: Respuesta completa ‚úÖ
- `stop_sequence`: Encontr√≥ secuencia de parada ‚úÖ
- `max_tokens`: Se alcanz√≥ el l√≠mite de tokens ‚ö†Ô∏è

```php
$response = $llm->exec();

$finishReason = $llm->getFinishReason();

if ($llm->isComplete()) {
    echo "‚úÖ Respuesta completa";
} else if (!$llm->wereTokenEnough()) {
    echo "‚ö†Ô∏è Se acabaron los tokens, aumenta max_tokens";
} else {
    echo "üö´ Respuesta bloqueada o interrumpida";
}
```

## Agregar Nuevos Proveedores

Para agregar un nuevo proveedor (por ejemplo, Google Gemini):

### 1. Crear el Provider

```php
// src/Providers/GeminiProvider.php
<?php

namespace Boctulus\LLMProviders\Providers;

use Boctulus\LLMProviders\Contracts\LLMProviderInterface;

class GeminiProvider implements LLMProviderInterface
{
    // Implementar todos los m√©todos de la interfaz
    public function exec(?string $model = null): array {
        // L√≥gica espec√≠fica de Gemini
    }

    // ... resto de m√©todos
}
```

### 2. Registrar en el Factory

```php
// src/Factory/LLMFactory.php

const PROVIDERS = [
    // ... proveedores existentes
    'gemini' => GeminiProvider::class,
    'google' => GeminiProvider::class,
];

// Agregar en instantiateProvider()
case GeminiProvider::class:
    $apiKey = $config['api_key'] ?? null;
    return new GeminiProvider($apiKey);

// M√©todo helper opcional
public static function gemini(?string $apiKey = null): GeminiProvider
{
    return new GeminiProvider($apiKey);
}
```

### 3. Usar el nuevo provider

```php
$llm = LLMFactory::create('gemini');
$llm->addContent('Hello Gemini!');
$response = $llm->exec();
```

## Arquitectura del Package

```
packages/boctulus/llm-providers/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ Contracts/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ LLMProviderInterface.php    # Interfaz com√∫n
‚îÇ   ‚îú‚îÄ‚îÄ Providers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ OpenAIProvider.php          # Implementaci√≥n OpenAI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ClaudeProvider.php          # Implementaci√≥n Claude
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GrokProvider.php            # Implementaci√≥n Grok
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ OllamaProvider.php          # Implementaci√≥n Ollama
‚îÇ   ‚îú‚îÄ‚îÄ Factory/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ LLMFactory.php              # Factory pattern
‚îÇ   ‚îú‚îÄ‚îÄ Exceptions/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ProviderException.php       # Excepciones personalizadas
‚îÇ   ‚îî‚îÄ‚îÄ ServiceProvider.php             # Service provider
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.php                      # Configuraci√≥n
‚îú‚îÄ‚îÄ composer.json
‚îî‚îÄ‚îÄ README.md
```

## Testing (Recomendado)

Aunque no incluye tests por defecto, se recomienda agregar:

```php
// tests/OpenAIProviderTest.php
use PHPUnit\Framework\TestCase;
use Boctulus\LLMProviders\Factory\LLMFactory;

class OpenAIProviderTest extends TestCase
{
    public function testBasicCompletion()
    {
        $llm = LLMFactory::openai();
        $llm->addContent('Say "test"');
        $response = $llm->exec();

        $this->assertEquals(200, $response['status']);
        $this->assertIsString($llm->getContent());
    }
}
```

## Troubleshooting

### API Key no encontrada

```
ProviderException: openai_api_key is required
```

**Soluci√≥n**: Configura la API key en `config/config.php` o p√°sala al constructor:

```php
$llm = LLMFactory::create('openai', ['api_key' => 'sk-...']);
```

### Respuesta vac√≠a

Si `getContent()` retorna `null`, verifica:

```php
$response = $llm->exec();

// 1. Verificar status HTTP
if ($response['status'] !== 200) {
    echo "Error HTTP: " . $response['status'];
}

// 2. Verificar errores del proveedor
if ($error = $llm->error()) {
    echo "Error: ";
    print_r($error);
}

// 3. Verificar estructura de respuesta
print_r($response['data']);
```

### Tokens insuficientes

```php
if (!$llm->wereTokenEnough()) {
    echo "Aumenta max_tokens";
    $llm->setMaxTokens(2000); // Ajustar
}
```

## Comparaci√≥n de Costos (Aproximados)

| Proveedor | Modelo | Input (1M tokens) | Output (1M tokens) |
|-----------|--------|-------------------|-------------------|
| OpenAI | gpt-4o-mini | $0.15 | $0.60 |
| OpenAI | gpt-4o | $2.50 | $10.00 |
| OpenAI | gpt-4 | $30.00 | $60.00 |
| Claude | haiku | $0.25 | $1.25 |
| Claude | sonnet | $3.00 | $15.00 |
| Claude | opus | $15.00 | $75.00 |
| Grok | grok-beta | TBD | TBD |

*Precios aproximados al 2025-01, sujetos a cambio*

## Contribuir

Para contribuir al package:

1. Implementa nuevos proveedores siguiendo la interfaz `LLMProviderInterface`
2. Agrega tests para tus providers
3. Actualiza la documentaci√≥n
4. Documenta modelos soportados y sus capacidades

## Licencia

MIT License

## Autor

**boctulus**

---

## Changelog

### v1.0.0 (2025-01)

- ‚úÖ Implementaci√≥n inicial con OpenAI, Claude y Grok
- ‚úÖ Factory pattern para instanciaci√≥n
- ‚úÖ Interfaz unificada LLMProviderInterface
- ‚úÖ Manejo robusto de errores con ProviderException
- ‚úÖ Soporte para extracci√≥n autom√°tica de JSON
- ‚úÖ Documentaci√≥n completa
- ‚úÖ Ejemplos de uso para cada proveedor
